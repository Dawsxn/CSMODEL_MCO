{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c42ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002042e4",
   "metadata": {},
   "source": [
    "# TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140b84a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0af8e",
   "metadata": {},
   "source": [
    "## Table of Contents üìë\n",
    "- [Research Question](#research-question)\n",
    "- [Dataset](#dataset)\n",
    "- [Data Cleaning](#data-cleaning)\n",
    "- [Data Preprocessing](#data-preprocessing)\n",
    "- [Exploratory Data Analysis](#exploratory-data-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373e481",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c1998",
   "metadata": {},
   "source": [
    "## Research Question ‚ùì <a id=\"research-question\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e473970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff17ebc4",
   "metadata": {},
   "source": [
    "[Back to Top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ac461",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8697287d",
   "metadata": {},
   "source": [
    "## Dataset üìä <a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331d8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"truthfulqa_responses.csv\", dtype={'start_time_epoch_s': float, 'end_time_epoch_s': float})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471aae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f04932",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce76a50",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28dc97b",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe50c5",
   "metadata": {},
   "source": [
    "[Back to Top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ae662",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad475c60",
   "metadata": {},
   "source": [
    "## Data Cleaning üßπ<a id=\"data-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28a7ca",
   "metadata": {},
   "source": [
    "Looking at the information below, we know that the total amount of rows initially is `23700`. Knowing this, we can see which rows have `null` values, which will be our first main target columns to be cleaned. In this case, we can see these columns are `response` and `source`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a69ac29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                               0\n",
       "category                           0\n",
       "question                           0\n",
       "correct_answer                     0\n",
       "incorrect_answer                   0\n",
       "correct_answer_label               0\n",
       "incorrect_answer_label             0\n",
       "source                             0\n",
       "start_time_epoch_s                 0\n",
       "end_time_epoch_s                   0\n",
       "model                              0\n",
       "input_tokens                       0\n",
       "output_tokens                      0\n",
       "input_price_per_million_tokens     0\n",
       "output_price_per_million_tokens    0\n",
       "system_prompt                      0\n",
       "user_prompt                        0\n",
       "response                           0\n",
       "language                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c660d",
   "metadata": {},
   "source": [
    "### Cleaning 'response' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fde974",
   "metadata": {},
   "source": [
    "To preserve the authenticity of each LLM's output, we aim to minimize modifications to the **`response`** column. The only cleaning applied here is replacing `NaN` values with `-1`, which serves as an indicator that the LLM gave **no response** or returned an **empty string**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b27e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb16bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'] = df['response'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df392294",
   "metadata": {},
   "source": [
    "### Cleaning 'source' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ca84f",
   "metadata": {},
   "source": [
    "For the `source` column, we chose to drop rows with `NaN` values since they make up only `60` out of `23,700` total rows. Additionally, rows without a `source` provide no verifiable reference for where the correct answer justification came from, making them less reliable for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f765664",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bf5e4",
   "metadata": {},
   "source": [
    "### Cleaning 'model' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f53a43",
   "metadata": {},
   "source": [
    "Aside from the columns with `NaN` values, we also decided to clean the `model` column. As observed from the unique values, the `gemini` model has an added prefix `\"models/\"`, which we will remove to maintain consistency across all entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03eb5d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deepseek-reasoner', 'models/gemini-2.5-pro-preview-05-06',\n",
       "       'o4-mini-2025-04-16'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ded8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model'] = df['model'].replace({'models/gemini-2.5-pro-preview-05-06': 'gemini-2.5-pro-preview-05-06'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec327e7",
   "metadata": {},
   "source": [
    "[Back to Top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0741423",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257223d",
   "metadata": {},
   "source": [
    "## Data Preprocessing üîß <a id=\"data-preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa1c58",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056f07c",
   "metadata": {},
   "source": [
    "The first column we will add is `latency`. This represents the total time it took for each LLM to respond ‚Äî more specifically, the duration of the API call for a specific question. To calculate this, we subtract `start_time_epoch_s` from `end_time_epoch_s`. The resulting value is in seconds and will be rounded to 4 decimal places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['latency'] = (df['end_time_epoch_s'] - df['start_time_epoch_s']).round(4)\n",
    "df['latency']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703205e",
   "metadata": {},
   "source": [
    "The second column we will add is `is_follow`. This will be a boolean value representing whether the LLM strictly followed the system prompt, regardless of the language. Since both prompts require the LLM to output only the letter of their answer, we determine this by checking if the `response` is one of the following values: `\"A\"`, `\"A.\"`, `\"B\"`, or `\"B.\"`. Rows with a `response` value of `-1` will be considered as not following the system prompt, as the prompt expects an answer but none was provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65f08725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_follow\n",
       "True     23575\n",
       "False       65\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_follow'] = df['response'].isin([\"A\", \"A.\", \"B\", \"B.\"])\n",
    "df['is_follow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dac553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "         ... \n",
       "23695    True\n",
       "23696    True\n",
       "23697    True\n",
       "23698    True\n",
       "23699    True\n",
       "Name: iscorrect, Length: 23632, dtype: bool"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['iscorrect'] = (df['response'] == df['correct_answer_label'])\n",
    "df['iscorrect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a313de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['total_input_price'] = (df['input_tokens'] / 1_000_000) * df['input_price_per_million_tokens']\n",
    "\n",
    "df['total_output_price'] = (df['output_tokens'] / 1_000_000) * df['output_price_per_million_tokens']\n",
    "\n",
    "df['total_price'] = df['total_input_price'] + df['total_output_price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d2b2b",
   "metadata": {},
   "source": [
    "[Back to Top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0afe0e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbaaed",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis üìà <a id=\"exploratory-data-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ce433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8e8c5e",
   "metadata": {},
   "source": [
    "[Back to Top](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c2d2c",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
