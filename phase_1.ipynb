{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c42ff7",
      "metadata": {
        "id": "49c42ff7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_dark\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002042e4",
      "metadata": {
        "id": "002042e4"
      },
      "source": [
        "# TITLE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b140b84a",
      "metadata": {
        "id": "b140b84a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d0af8e",
      "metadata": {
        "id": "89d0af8e"
      },
      "source": [
        "## Table of Contents üìë"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0999f0fa",
      "metadata": {},
      "source": [
        "- [Research Question](#research-question)\n",
        "- [Dataset](#dataset)\n",
        "- [Data Cleaning](#data-cleaning)\n",
        "- [Data Preprocessing](#data-preprocessing)\n",
        "- [Exploratory Data Analysis](#exploratory-data-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a373e481",
      "metadata": {
        "id": "a373e481"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4c1998",
      "metadata": {
        "id": "5a4c1998"
      },
      "source": [
        "## Research Question ‚ùì <a id=\"research-question\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e473970",
      "metadata": {
        "id": "4e473970"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ff17ebc4",
      "metadata": {
        "id": "ff17ebc4"
      },
      "source": [
        "[Back to Top](#title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8ac461",
      "metadata": {
        "id": "db8ac461"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8697287d",
      "metadata": {
        "id": "8697287d"
      },
      "source": [
        "## Dataset üìä <a id=\"dataset\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331d8005",
      "metadata": {
        "id": "331d8005"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"truthfulqa_responses.csv\", dtype={'start_time_epoch_s': float, 'end_time_epoch_s': float})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471aae4b",
      "metadata": {
        "id": "471aae4b",
        "outputId": "d09b09e7-7bdb-4471-a8fb-414c336e1ec2"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4f04932",
      "metadata": {
        "id": "a4f04932"
      },
      "source": [
        "### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y2iaRq8BoVM8",
      "metadata": {
        "id": "y2iaRq8BoVM8"
      },
      "source": [
        "This dataset looks at how free-tier large language models respond to prompts that test their ability to avoid repeating false but common human beliefs. It includes answers from three models: o4-mini from OpenAI, DeepSeek-R1 from DeepSeek, and Gemini 2.5 Pro from Google. The prompts come from the TruthfulQA benchmark, which focuses on whether a model can give factually correct answers instead of ones that just sound right. Each response is saved with details like the question asked, the model that answered, and a label showing if the response was true, false, or uncertain. The setup makes it easier to compare models and see patterns in how they deal with truthfulness, especially when it comes to misleading but familiar ideas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce76a50",
      "metadata": {
        "id": "1ce76a50"
      },
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kRBGyUuJpEvx",
      "metadata": {
        "id": "kRBGyUuJpEvx"
      },
      "source": [
        "The same set of TruthfulQA prompts was sent to each model using their official APIs. Responses were collected in a consistent and automated way, with each one saved along with the prompt, the model that answered, and a label showing if the response was true or not. The process followed platform rules and made sure the data was collected properly and handled with care. However, since the models were accessed through different APIs and may have slight differences in settings or response formatting, these factors could affect how the outputs are interpreted. The way truthfulness is labeled may also involve some level of subjectivity, especially for prompts that are vague or open-ended. These aspects should be considered when analyzing the results and drawing conclusions from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28dc97b",
      "metadata": {
        "id": "d28dc97b"
      },
      "source": [
        "### Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YOG_zFBkpUZd",
      "metadata": {
        "id": "YOG_zFBkpUZd"
      },
      "source": [
        "The dataset is structured as a table, with each row representing one model's response to a TruthfulQA question. There are 23,700 observations and 21 columns in total. Each observation includes metadata about the prompt, the model‚Äôs response, and additional details relevant to performance analysis and cost tracking.\n",
        "\n",
        "The key attributes in each observation are as follows:\n",
        "\n",
        "*   **type** ‚Äì identifies whether the prompt is a truthful or misleading question.\n",
        "*   **category** ‚Äì the topic of the question, such as health, science, or history.\n",
        "\n",
        "* **question** ‚Äì the full question text from the TruthfulQA benchmark.\n",
        "\n",
        "* **correct_answer** ‚Äì the factually accurate answer to the question.\n",
        "\n",
        "* **incorrect_answer** ‚Äì a commonly believed but false response to the question.\n",
        "\n",
        "* **correct_answer_label** ‚Äì a tag or label marking the correct answer.\n",
        "\n",
        "* **incorrect_answer_label** ‚Äì a tag or label marking the incorrect answer.\n",
        "\n",
        "* **source** ‚Äì the original source of the question or prompt.\n",
        "\n",
        "* **start_time_epoch_s** and **end_time_epoch_s** ‚Äì timestamps (in epoch seconds) marking when the model request started and ended.\n",
        "\n",
        "* **model** ‚Äì the name of the language model that generated the response (e.g., o4-mini, DeepSeek-R1, Gemini 2.5 Pro).\n",
        "\n",
        "* **input_tokens** and **output_tokens** ‚Äì the number of tokens used in the input and generated in the output.\n",
        "\n",
        "* **input_price_per_million_tokens** and **output_price_per_million_tokens** ‚Äì estimated cost per million tokens for input and output, based on model pricing.\n",
        "\n",
        "* **system_prompt** ‚Äì the system-level instruction provided to the model.\n",
        "\n",
        "* **user_prompt** ‚Äì the prompt sent to the model, typically the question text.\n",
        "\n",
        "* **response** ‚Äì the actual answer generated by the model.\n",
        "\n",
        "* **language** ‚Äì the language in which the model responded.\n",
        "\n",
        "\n",
        "Each observation includes both input and output data that can be analyzed to compare model behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ebe50c5",
      "metadata": {
        "id": "1ebe50c5"
      },
      "source": [
        "[Back to Top](#title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "316ae662",
      "metadata": {
        "id": "316ae662"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad475c60",
      "metadata": {
        "id": "ad475c60"
      },
      "source": [
        "## Data Cleaning üßπ<a id=\"data-cleaning\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc28a7ca",
      "metadata": {
        "id": "fc28a7ca"
      },
      "source": [
        "Looking at the information below, we know that the total amount of rows initially is `23700`. Knowing this, we can see which rows have `null` values, which will be our first main target columns to be cleaned. In this case, we can see these columns are `response` and `source`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dac9f6a",
      "metadata": {
        "id": "9dac9f6a",
        "outputId": "f1f976cb-aeb0-4b79-ae95-75007e679a80"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "df.head()\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69ac29e",
      "metadata": {
        "id": "a69ac29e",
        "outputId": "c6e61df2-fbf6-422a-c939-27dc60ca7a33"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114c660d",
      "metadata": {
        "id": "114c660d"
      },
      "source": [
        "### Cleaning 'response' column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fde974",
      "metadata": {
        "id": "01fde974"
      },
      "source": [
        "To preserve the authenticity of each LLM's output, we aim to minimize modifications to the **`response`** column. The only cleaning applied here is replacing `NaN` values with `-1`, which serves as an indicator that the LLM gave **no response** or returned an **empty string**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37b27e1e",
      "metadata": {
        "id": "37b27e1e",
        "outputId": "adf6a831-d767-4667-cf0a-1265931dced2"
      },
      "outputs": [],
      "source": [
        "df['response'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb16bb54",
      "metadata": {
        "id": "fb16bb54"
      },
      "outputs": [],
      "source": [
        "df['response'] = df['response'].fillna(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df392294",
      "metadata": {
        "id": "df392294"
      },
      "source": [
        "### Cleaning 'source' column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54ca84f",
      "metadata": {
        "id": "e54ca84f"
      },
      "source": [
        "For the `source` column, we chose to drop rows with `NaN` values since they make up only `60` out of `23,700` total rows. Additionally, rows without a `source` provide no verifiable reference for where the correct answer justification came from, making them less reliable for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f765664",
      "metadata": {
        "id": "6f765664"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['source'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5bf5e4",
      "metadata": {
        "id": "7d5bf5e4"
      },
      "source": [
        "### Cleaning 'model' column"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f53a43",
      "metadata": {
        "id": "00f53a43"
      },
      "source": [
        "Aside from the columns with `NaN` values, we also decided to clean the `model` column. As observed from the unique values, the `gemini` model has an added prefix `\"models/\"`, which we will remove to maintain consistency across all entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03eb5d49",
      "metadata": {
        "id": "03eb5d49",
        "outputId": "b8a30088-95ee-4efc-f6be-67ac43849d37"
      },
      "outputs": [],
      "source": [
        "df['model'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ded8bb",
      "metadata": {
        "id": "b6ded8bb",
        "outputId": "411dfbe1-b91f-498d-a42b-09be4b98ad6d"
      },
      "outputs": [],
      "source": [
        "df['model'] = df['model'].replace({'models/gemini-2.5-pro-preview-05-06': 'gemini-2.5-pro-preview-05-06'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec327e7",
      "metadata": {
        "id": "6ec327e7"
      },
      "source": [
        "[Back to Top](#title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0741423",
      "metadata": {
        "id": "d0741423"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f257223d",
      "metadata": {
        "id": "f257223d"
      },
      "source": [
        "## Data Preprocessing üîß <a id=\"data-preprocessing\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8caa1c58",
      "metadata": {
        "id": "8caa1c58"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b056f07c",
      "metadata": {
        "id": "b056f07c"
      },
      "source": [
        "The first column we will add is `latency`. This represents the total time it took for each LLM to respond ‚Äî more specifically, the duration of the API call for a specific question. To calculate this, we subtract `start_time_epoch_s` from `end_time_epoch_s`. The resulting value is in seconds and will be rounded to 4 decimal places.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f3d85e5",
      "metadata": {
        "id": "8f3d85e5",
        "outputId": "67a4f1f8-6095-45bc-f872-6a545a880ec4"
      },
      "outputs": [],
      "source": [
        "df['latency'] = (df['end_time_epoch_s'] - df['start_time_epoch_s']).round(4)\n",
        "df['latency']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6703205e",
      "metadata": {
        "id": "6703205e"
      },
      "source": [
        "The second column we will add is `is_follow`. This will be a boolean value representing whether the LLM strictly followed the system prompt, regardless of the language. Since both prompts require the LLM to output only the letter of their answer, we determine this by checking if the `response` is one of the following values: `\"A\"` or `\"B\"`. Rows with a `response` value of `-1` will be considered as not following the system prompt, as the prompt expects an answer but none was provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f08725",
      "metadata": {
        "id": "65f08725",
        "outputId": "23a1db23-3492-49f9-930e-9ccefa3b8c2a"
      },
      "outputs": [],
      "source": [
        "df['is_follow'] = df['response'].isin([\"A\", \"B\"])\n",
        "df['is_follow'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78dfb8b",
      "metadata": {
        "id": "c78dfb8b"
      },
      "source": [
        "The third column we will add is `is_correct`. This will be a boolean value represnting whether the LLM provided the correct answer. While a straightforward way to determine this is by comparing the `response` column with the `correct_answer_label` column, we need to keep in mind that some rows do not follow the system prompt of strictly outputting only the answer letter. These irregular responses could be unpredictable, so we will handle these cases first.\n",
        "\n",
        "To address this, we will investigate and use the `is_follow` column to identify which rows did not strictly follow the prompt. Then, we will examine the values in their `response` column to determine how to handle the irregular responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a221af",
      "metadata": {
        "id": "63a221af",
        "outputId": "1e53079f-b80f-4a81-d432-dd18d5bb67f6"
      },
      "outputs": [],
      "source": [
        "df['is_correct'] = pd.NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881c4091",
      "metadata": {
        "id": "881c4091",
        "outputId": "fe39e72f-f8e6-4b22-f738-3b226a9f9fa7"
      },
      "outputs": [],
      "source": [
        "is_follow_false = (df[df['is_follow'] == False])\n",
        "is_follow_false['response'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45a505f5",
      "metadata": {
        "id": "45a505f5"
      },
      "source": [
        "After observing the values, we noticed that the majority follow a similar format: `\"Letter of Choice: Choice\"`. However, there are a few exceptions ‚Äî specifically three distinct values: `-1`, `\"Sagot: A\"`, and `\"Pasensya na, hindi ko masagot iyan.\"`. We will first address the latter two cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6d05b2",
      "metadata": {
        "id": "8d6d05b2"
      },
      "outputs": [],
      "source": [
        "df[df['response'] == 'Sagot: A']\n",
        "df.loc[df['response'] == 'Sagot: A', 'is_correct'] = True\n",
        "\n",
        "df.loc[df['response'] == 'Pasensya na, hindi ko masagot iyan.', 'is_correct'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd064972",
      "metadata": {
        "id": "dd064972"
      },
      "source": [
        "Now that we have addressed the two special cases, we can proceed to set the values for the rest of the rows by simply comparing the **first character** of each `response` to the `correct_answer_label`. This method conveniently includes edge cases like responses equal to `-1`, which will be treated as incorrect since the first character will not match any valid label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dac553",
      "metadata": {
        "id": "e4dac553"
      },
      "outputs": [],
      "source": [
        "mask = df['is_correct'].isna()\n",
        "\n",
        "df.loc[mask, 'is_correct'] = (\n",
        "    df.loc[mask, 'response'].str[0] == df.loc[mask, 'correct_answer_label']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9df6a8d",
      "metadata": {
        "id": "a9df6a8d"
      },
      "source": [
        "The next few columns we will be adding are:\n",
        "\n",
        "- `total_input_price`: the total amount spent for input tokens for that row (in dollars)\n",
        "- `total_output_price`: the total amount spent for output tokens for that row (in dollars)\n",
        "- `total_price`: the total amount spent for all tokens for that row (in dollars)\n",
        "\n",
        "To compute these values, we will use the following columns:\n",
        "- `input_tokens`\n",
        "- `output_tokens`\n",
        "- `input_price_per_million_tokens`\n",
        "- `output_price_per_million_tokens`\n",
        "\n",
        "Each token cost is priced per million tokens, so we will divide the token counts by 1,000,000 and multiply by their respective price rates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a313de1a",
      "metadata": {
        "id": "a313de1a",
        "outputId": "cd8eebb0-babd-4b78-b043-f0f8e938f7a3"
      },
      "outputs": [],
      "source": [
        "\n",
        "df['total_input_price'] = (df['input_tokens'] / 1_000_000) * df['input_price_per_million_tokens']\n",
        "\n",
        "df['total_output_price'] = (df['output_tokens'] / 1_000_000) * df['output_price_per_million_tokens']\n",
        "\n",
        "df['total_price'] = df['total_input_price'] + df['total_output_price']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f21961a",
      "metadata": {},
      "source": [
        "Now, although we have the `input` and `output tokens` column, it is important to note that each `LLM` has a different way of `tokenizing`. This suggests that we should standardize these columns according to their respective models using `z-score standardization`, so that we can fairly compare `token usage` across different models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf685c47",
      "metadata": {},
      "outputs": [],
      "source": [
        "df[['input_tokens_z', 'output_tokens_z']] = df.groupby('model')[['input_tokens', 'output_tokens']].transform(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416fd858",
      "metadata": {
        "id": "416fd858"
      },
      "source": [
        "Let us take a quick look at our dataset after adding all these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "104eb2c6",
      "metadata": {
        "id": "104eb2c6",
        "outputId": "720b2677-2b89-482d-821b-e0a5a19e25e2"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842d2b2b",
      "metadata": {
        "id": "842d2b2b"
      },
      "source": [
        "[Back to Top](#title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0afe0e",
      "metadata": {
        "id": "ad0afe0e"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3dbaaed",
      "metadata": {
        "id": "f3dbaaed"
      },
      "source": [
        "## Exploratory Data Analysis üìà <a id=\"exploratory-data-analysis\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43535abf",
      "metadata": {},
      "source": [
        "### Which factors correlate with the accuracy of large language models when answering prompts designed to mimic human misconceptions?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e5020f",
      "metadata": {},
      "source": [
        "#### What is the accuracy of current free-tier reasoning large language models on adversarial and non-adversarial questions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe38922",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "type_accuracy = df.groupby('type')['is_correct'].mean().reset_index()\n",
        "type_accuracy['accuracy_percent'] = type_accuracy['is_correct'] * 100\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(\n",
        "    type_accuracy,\n",
        "    x='type',\n",
        "    y='accuracy_percent',\n",
        "    title='Accuracy of Free-Tier LLMs on Adversarial vs Non-Adversarial Questions',\n",
        "    text='accuracy_percent',\n",
        "    labels={'type': 'Question Type', 'accuracy_percent': 'Accuracy (%)'},\n",
        ")\n",
        "\n",
        "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
        "fig.update_layout(yaxis_range=[0, 100])\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d88393",
      "metadata": {},
      "source": [
        "#### What is the accuracy of current free-tier reasoning large language models on different question categories?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c4890e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "type_accuracy = df.groupby('category')['is_correct'].mean().reset_index()\n",
        "type_accuracy['accuracy_percent'] = type_accuracy['is_correct'] * 100\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(\n",
        "    type_accuracy,\n",
        "    x='category',\n",
        "    y='accuracy_percent',\n",
        "    title='Accuracy of Free-Tier LLMs on Different Question Categories',\n",
        "    text='accuracy_percent',\n",
        "    labels={'category': 'Question Category', 'accuracy_percent': 'Accuracy (%)'},\n",
        ")\n",
        "\n",
        "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
        "fig.update_layout(yaxis_range=[0, 100])\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911f9344",
      "metadata": {},
      "source": [
        "#### What is the accuracy of current free-tier reasoning large language models on English and Filipino?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac844650",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "type_accuracy = df.groupby('language')['is_correct'].mean().reset_index()\n",
        "type_accuracy['accuracy_percent'] = type_accuracy['is_correct'] * 100\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(\n",
        "    type_accuracy,\n",
        "    x='language',\n",
        "    y='accuracy_percent',\n",
        "    title='Accuracy of Free-Tier LLMs on Different Question Categories',\n",
        "    text='accuracy_percent',\n",
        "    labels={'language': 'Question language', 'accuracy_percent': 'Accuracy (%)'},\n",
        ")\n",
        "\n",
        "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
        "fig.update_layout(yaxis_range=[0, 100])\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c497423e",
      "metadata": {},
      "source": [
        "### How do free-tier large language models compare, on different languages, in terms of performance on truthfulness benchmarks?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "267eabcb",
      "metadata": {},
      "source": [
        "#### Which model has the highest accuracy on TruthfulQA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2e1b41",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Normalize language capitalization (important!)\n",
        "df['language'] = df['language'].str.capitalize()\n",
        "\n",
        "# Group by language and model\n",
        "grouped_lang = df.groupby(['language', 'model'])['is_correct'].mean().reset_index()\n",
        "grouped_lang['accuracy_percent'] = grouped_lang['is_correct'] * 100\n",
        "\n",
        "# Group by model only (combined)\n",
        "grouped_combined = df.groupby('model')['is_correct'].mean().reset_index()\n",
        "grouped_combined['accuracy_percent'] = grouped_combined['is_correct'] * 100\n",
        "grouped_combined['language'] = 'English and Filipino'  # Add synthetic 'language'\n",
        "\n",
        "# Combine into one DataFrame\n",
        "grouped_all = pd.concat([grouped_lang, grouped_combined], ignore_index=True)\n",
        "\n",
        "# Desired subplot order\n",
        "languages = ['English', 'Filipino', 'English and Filipino']\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=languages,\n",
        "    shared_yaxes=True\n",
        ")\n",
        "\n",
        "# Add bar chart for each language section\n",
        "for i, lang in enumerate(languages):\n",
        "    lang_data = grouped_all[grouped_all['language'] == lang]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=lang_data['model'],\n",
        "            y=lang_data['accuracy_percent'],\n",
        "            text=lang_data['accuracy_percent'].apply(lambda x: f'{x:.2f}%'),\n",
        "            textposition='outside',\n",
        "        ),\n",
        "        row=1, col=i+1\n",
        "    )\n",
        "\n",
        "# Layout config\n",
        "fig.update_layout(\n",
        "    title_text=\"Accuracy of Free-Tier LLMs on English, Filipino, and Combined Questions\",\n",
        "    showlegend=False,\n",
        "    template=\"plotly_dark\"\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "# Keep Y-axes consistent\n",
        "for i in range(3):\n",
        "    fig.update_yaxes(range=[0, 100], row=1, col=i+1)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "173a0a81",
      "metadata": {},
      "source": [
        "#### Which model has the least latency on TruthfulQA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3337b560",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Normalize language capitalization\n",
        "df['language'] = df['language'].str.capitalize()\n",
        "\n",
        "# Group by language and model - latency\n",
        "grouped_lang = df.groupby(['language', 'model'])['latency'].mean().reset_index()\n",
        "grouped_lang['latency'] = grouped_lang['latency']\n",
        "\n",
        "# Group by model only (combined latency)\n",
        "grouped_combined = df.groupby('model')['latency'].mean().reset_index()\n",
        "grouped_combined['language'] = 'English and Filipino'  # synthetic 'language'\n",
        "\n",
        "# Combine into one DataFrame\n",
        "grouped_all = pd.concat([grouped_lang, grouped_combined], ignore_index=True)\n",
        "\n",
        "# Desired subplot order\n",
        "languages = ['English', 'Filipino', 'English and Filipino']\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=languages,\n",
        "    shared_yaxes=True\n",
        ")\n",
        "\n",
        "# Add bar chart for each language section\n",
        "for i, lang in enumerate(languages):\n",
        "    lang_data = grouped_all[grouped_all['language'] == lang]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=lang_data['model'],\n",
        "            y=lang_data['latency'],\n",
        "            text=lang_data['latency'].apply(lambda x: f'{x:.2f}'),\n",
        "            textposition='outside',\n",
        "        ),\n",
        "        row=1, col=i+1\n",
        "    )\n",
        "\n",
        "# Layout config\n",
        "fig.update_layout(\n",
        "    title_text=\"Average Latency of Free-Tier LLMs on English, Filipino, and Combined Questions\",\n",
        "    showlegend=False,\n",
        "    template=\"plotly_dark\",\n",
        "    yaxis_title=\"Latency (s)\"  # change if your latency is in ms\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "# Optional: adjust Y-axis range manually if desired\n",
        "# for i in range(3):\n",
        "#     fig.update_yaxes(range=[0, 5], row=1, col=i+1)  # or whatever max latency is\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd2131f",
      "metadata": {},
      "source": [
        "#### Which model has the least cost on TruthfulQA? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da53bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Normalize language capitalization\n",
        "df['language'] = df['language'].str.capitalize()\n",
        "\n",
        "# Group by language and model ‚Äî SUM of total_price\n",
        "grouped_lang = df.groupby(['language', 'model'])['total_price'].sum().reset_index()\n",
        "\n",
        "# Group by model only (combined cost)\n",
        "grouped_combined = df.groupby('model')['total_price'].sum().reset_index()\n",
        "grouped_combined['language'] = 'English and Filipino'  # synthetic category\n",
        "\n",
        "# Combine all\n",
        "grouped_all = pd.concat([grouped_lang, grouped_combined], ignore_index=True)\n",
        "\n",
        "# Define subplot layout\n",
        "languages = ['English', 'Filipino', 'English and Filipino']\n",
        "\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=languages,\n",
        "    shared_yaxes=True\n",
        ")\n",
        "\n",
        "# Add bar plots per language\n",
        "for i, lang in enumerate(languages):\n",
        "    lang_data = grouped_all[grouped_all['language'] == lang]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=lang_data['model'],\n",
        "            y=lang_data['total_price'],\n",
        "            text=lang_data['total_price'].apply(lambda x: f'${x:.4f}'),\n",
        "            textposition='outside',\n",
        "        ),\n",
        "        row=1, col=i+1\n",
        "    )\n",
        "\n",
        "# Layout and labels\n",
        "fig.update_layout(\n",
        "    title_text=\"Total Cost of Free-Tier LLMs on TruthfulQA (Summed Price)\",\n",
        "    showlegend=False,\n",
        "    template=\"plotly_dark\",\n",
        "    yaxis_title=\"Total Cost (USD)\"\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f2b8769",
      "metadata": {},
      "source": [
        "#### Which model follows instructions the best?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ee1ab4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Normalize language capitalization\n",
        "df['language'] = df['language'].str.capitalize()\n",
        "\n",
        "# Group by language and model ‚Äî mean is_follow\n",
        "grouped_lang = df.groupby(['language', 'model'])['is_follow'].mean().reset_index()\n",
        "grouped_lang['is_follow_percent'] = grouped_lang['is_follow'] * 100\n",
        "\n",
        "# Group by model only (combined)\n",
        "grouped_combined = df.groupby('model')['is_follow'].mean().reset_index()\n",
        "grouped_combined['is_follow_percent'] = grouped_combined['is_follow'] * 100\n",
        "grouped_combined['language'] = 'English and Filipino'\n",
        "\n",
        "# Combine everything\n",
        "grouped_all = pd.concat([grouped_lang, grouped_combined], ignore_index=True)\n",
        "\n",
        "# Define subplot categories\n",
        "languages = ['English', 'Filipino', 'English and Filipino']\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=languages,\n",
        "    shared_yaxes=True\n",
        ")\n",
        "\n",
        "# Add bar plots per language\n",
        "for i, lang in enumerate(languages):\n",
        "    lang_data = grouped_all[grouped_all['language'] == lang]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=lang_data['model'],\n",
        "            y=lang_data['is_follow_percent'],\n",
        "            text=lang_data['is_follow_percent'].apply(lambda x: f'{x:.2f}%'),\n",
        "            textposition='outside',\n",
        "        ),\n",
        "        row=1, col=i+1\n",
        "    )\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Instruction Following of Free-Tier LLMs on English, Filipino, and Combined Questions\",\n",
        "    showlegend=False,\n",
        "    template=\"plotly_dark\",\n",
        "    yaxis_title=\"Following Instructions (%)\"\n",
        ")\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "# Keep y-axis in [0, 100]\n",
        "for i in range(3):\n",
        "    fig.update_yaxes(range=[0, 100], row=1, col=i+1)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f124aca",
      "metadata": {},
      "source": [
        "#### Which model has the most verbose reasoning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f30486",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Normalize language capitalization\n",
        "df['language'] = df['language'].str.capitalize()\n",
        "\n",
        "# Group by language and model ‚Äî mean output_tokens_z\n",
        "grouped_lang = df.groupby(['language', 'model'])['output_tokens_z'].mean().reset_index()\n",
        "\n",
        "# Group by model only (combined)\n",
        "grouped_combined = df.groupby('model')['output_tokens_z'].mean().reset_index()\n",
        "grouped_combined['language'] = 'English and Filipino'\n",
        "\n",
        "# Combine\n",
        "grouped_all = pd.concat([grouped_lang, grouped_combined], ignore_index=True)\n",
        "\n",
        "# Define subplot categories\n",
        "languages = ['English', 'Filipino', 'English and Filipino']\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=3,\n",
        "    subplot_titles=languages,\n",
        "    shared_yaxes=True\n",
        ")\n",
        "\n",
        "# Add bar charts\n",
        "for i, lang in enumerate(languages):\n",
        "    lang_data = grouped_all[grouped_all['language'] == lang]\n",
        "    fig.add_trace(\n",
        "        go.Bar(\n",
        "            x=lang_data['model'],\n",
        "            y=lang_data['output_tokens_z'],\n",
        "            text=lang_data['output_tokens_z'].apply(lambda x: f'{x:.2f}'),\n",
        "            textposition='outside',\n",
        "        ),\n",
        "        row=1, col=i+1\n",
        "    )\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(\n",
        "    title_text=\"Mean Z-Score of Output Tokens by Free-Tier LLMs on English, Filipino, and Combined Questions\",\n",
        "    showlegend=False,\n",
        "    template=\"plotly_dark\",\n",
        "    yaxis_title=\"Mean Output Token Z-Score\"\n",
        ")\n",
        "fig.update_yaxes(zeroline=True, zerolinewidth=2, zerolinecolor='white')\n",
        "\n",
        "\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c8e8c5e",
      "metadata": {
        "id": "5c8e8c5e"
      },
      "source": [
        "[Back to Top](#title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685c2d2c",
      "metadata": {
        "id": "685c2d2c"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
